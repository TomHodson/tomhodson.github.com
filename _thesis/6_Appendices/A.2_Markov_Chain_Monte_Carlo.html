---
title: Markov Chain Monte Carlo
excerpt: 
layout: none
image: 

---
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Markov Chain Monte Carlo</title>


<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
  "HTML-CSS": {
    linebreaks: { automatic: true, width: "container" }          
  }              
  });
  
</script>
<script src="/assets/mathjax/tex-mml-svg.js" id="MathJax-script" async></script>

<script src="/assets/js/thesis_scrollspy.js"></script>
<script src="https://d3js.org/d3.v5.min.js" defer></script>

<link rel="stylesheet" href="/assets/css/styles.css">
<script src="/assets/js/index.js"></script>
</head>
<body>

<!--Capture the table of contents from pandoc as a jekyll variable  -->
{% capture tableOfContents %}
<br>
<nav aria-label="Table of Contents" class="page-table-of-contents">
<ul>
<li><a href="#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo">Markov Chain Monte Carlo</a>
<ul>
<li><a href="#direct-random-sampling" id="toc-direct-random-sampling">Direct Random Sampling</a></li>
<li><a href="#mcmc-sampling" id="toc-mcmc-sampling">MCMC Sampling</a></li>
<li><a href="#implementation-of-mcmc" id="toc-implementation-of-mcmc">Implementation of MCMC</a></li>
<li><a href="#global-and-detailed-balance-equations" id="toc-global-and-detailed-balance-equations">Global and Detailed balance equations</a></li>
<li><a href="#the-metropolis-hastings-algorithm" id="toc-the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</a></li>
<li><a href="#app-mcmc-two-step-trick" id="toc-app-mcmc-two-step-trick">Two Step Trick</a>
<ul>
<li><a href="#app-mcmc-autocorrelation" id="toc-app-mcmc-autocorrelation">Autocorrelation Time</a></li>
<li><a href="#tuning-the-proposal-distribution" id="toc-tuning-the-proposal-distribution">Tuning the proposal distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#bibliography" id="toc-bibliography">Bibliography</a></li>
</ul>
</nav>
{% endcapture %}

<!-- Give the table of contents to header as a variable so it can be put into the sidebar-->
{% include header.html extra=tableOfContents %}

<main>

<!-- Table of Contents -->
<!-- <nav id="TOC" role="doc-toc">
<ul>
<li><a href="#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo">Markov Chain Monte Carlo</a>
<ul>
<li><a href="#direct-random-sampling" id="toc-direct-random-sampling">Direct Random Sampling</a></li>
<li><a href="#mcmc-sampling" id="toc-mcmc-sampling">MCMC Sampling</a></li>
<li><a href="#implementation-of-mcmc" id="toc-implementation-of-mcmc">Implementation of MCMC</a></li>
<li><a href="#global-and-detailed-balance-equations" id="toc-global-and-detailed-balance-equations">Global and Detailed balance equations</a></li>
<li><a href="#the-metropolis-hastings-algorithm" id="toc-the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</a></li>
<li><a href="#app-mcmc-two-step-trick" id="toc-app-mcmc-two-step-trick">Two Step Trick</a>
<ul>
<li><a href="#app-mcmc-autocorrelation" id="toc-app-mcmc-autocorrelation">Autocorrelation Time</a></li>
<li><a href="#tuning-the-proposal-distribution" id="toc-tuning-the-proposal-distribution">Tuning the proposal distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#bibliography" id="toc-bibliography">Bibliography</a></li>
</ul>
</nav>
 -->

<!-- Main Page Body -->
<div id="page-header">
<p>Appendices</p>
<hr />
</div>
<section id="markov-chain-monte-carlo" class="level1">
<h1>Markov Chain Monte Carlo</h1>
<p>Markov Chain Monte Carlo (MCMC) is a useful method whenever we have a probability distribution that we want to sample from but there is not direct sampling way to do so.</p>
<section id="direct-random-sampling" class="level2">
<h2>Direct Random Sampling</h2>
<p>In almost any computer simulation the ultimate source of randomness is a stream of (close to) uniform, uncorrelated bits generated from a pseudo random number generator. A direct sampling method takes such a source and outputs uncorrelated samples from the target distribution. The fact they are uncorrelated is key as we’ll see later. Examples of direct sampling methods range from the trivial: take n random bits to generate integers uniformly between 0 and <span class="math inline">\(2^n\)</span> to more complex methods such as inverse transform sampling and rejection sampling <span class="citation" data-cites="devroyeRandomSampling1986"> [<a href="#ref-devroyeRandomSampling1986" role="doc-biblioref">1</a>]</span>.</p>
<p>In physics the distribution we usually want to sample from is the Boltzmann probability over states of the system <span class="math inline">\(S\)</span>: <span class="math display">\[
\begin{aligned}
p(S)  &amp;= \frac{1}{\mathcal{Z}} e^{-\beta H(S)}, \\
\end{aligned}
\]</span> where <span class="math inline">\(\mathcal{Z} = \sum_S e^{-\beta H(S)}\)</span> is the normalisation factor and ubiquitous partition function. In principle we could directly sample from this, for a discrete system there are finitely many choices. We could calculate the probability of each one and assign each a region of the unit interval which we could then sample uniformly from.</p>
<p>However, if we actually try to do this we will run into two problems, we can’t calculate <span class="math inline">\(\mathcal{Z}\)</span> for any reasonably sized systems because the state space grows exponentially with system size. Even if we could calculate <span class="math inline">\(\mathcal{Z}\)</span>, sampling from an exponentially large number of options quickly become tricky. This kind of problem happens in many other disciplines too, particularly when fitting statistical models using Bayesian inference <span class="citation" data-cites="BMCP2021"> [<a href="#ref-BMCP2021" role="doc-biblioref">2</a>]</span>.</p>
</section>
<section id="mcmc-sampling" class="level2">
<h2>MCMC Sampling</h2>
<p>So what can we do? Well it turns out that if we are willing to give up in the requirement that the samples be uncorrelated then we can use MCMC instead.</p>
<p>MCMC defines a weighted random walk over the states <span class="math inline">\((S_0, S_1, S_2, ...)\)</span>, such that in the long time limit, states are visited according to their probability <span class="math inline">\(p(S)\)</span>. <span class="citation" data-cites="binderGuidePracticalWork1988 kerteszAdvancesComputerSimulation1998 wolffMonteCarloErrors2004"> [<a href="#ref-binderGuidePracticalWork1988" role="doc-biblioref">3</a>–<a href="#ref-wolffMonteCarloErrors2004" role="doc-biblioref">5</a>]</span>.  <span class="citation" data-cites="krauthIntroductionMonteCarlo1998"> [<a href="#ref-krauthIntroductionMonteCarlo1998" role="doc-biblioref">6</a>]</span></p>
<p><span class="math display">\[\lim_{i\to\infty} p(S_i) = P(S).\]</span></p>
<p>In a physics context this lets us evaluate any observable with a mean over the states visited by the walk. <span class="math display">\[\begin{aligned}
\langle O \rangle &amp; = \sum_{S} p(S) \langle O \rangle_{S} = \sum_{i = 0}^{M} \langle O\rangle_{S_i} + \mathcal{O}(\tfrac{1}{\sqrt{M}}).\\
\end{aligned}\]</span></p>
<p>The samples in the random walk are correlated so the samples effectively contain less information than <span class="math inline">\(N\)</span> independent samples would. As a consequence the variance is larger than the <span class="math inline">\(\langle O^2 \rangle - \langle O\rangle^2\)</span> form it would have if the estimates were uncorrelated. Methods of estimating the true variance of <span class="math inline">\(\langle O \rangle\)</span> and decided how many steps are needed will be considered later.</p>
</section>
<section id="implementation-of-mcmc" class="level2">
<h2>Implementation of MCMC</h2>
<p>In implementation MCMC can be boiled down to choosing a transition function <span class="math inline">\(\mathcal{T}(S_{t} \rightarrow S_{t+1})\)</span> where <span class="math inline">\(S\)</span> are vectors representing classical spin configurations. We start in some initial state <span class="math inline">\(S_0\)</span> and then repeatedly jump to new states according to the probabilities given by <span class="math inline">\(\mathcal{T}\)</span>. This defines a set of random walks <span class="math inline">\(\{S_0\ldots S_i\ldots S_N\}\)</span>.</p>
<p>In pseudo-code one could write the MCMC simulation for a single walker as:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A skeleton implementation of MCMC</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>current_state <span class="op">=</span> initial_state</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_steps):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    new_state <span class="op">=</span> sampleT(current_state) </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    states[i] <span class="op">=</span> current_state</span></code></pre></div>
<p>Where the <code>sampleT</code> function samples directly from the transition function <span class="math inline">\(\mathcal{T}\)</span>.</p>
<p>If we run many such walkers in parallel we can then approximate the distribution <span class="math inline">\(p_t(S; S)\)</span> which tells us where the walkers are likely to be after they’ve evolved for <span class="math inline">\(t\)</span> steps from an initial state <span class="math inline">\(S_0\)</span>. We need to carefully choose <span class="math inline">\(\mathcal{T}\)</span> such that the probability <span class="math inline">\(p_t(S; S_0)\)</span> approaches the distribution of interest. In this case the thermal distribution <span class="math inline">\(P(S; \beta) = \mathcal{Z}^{-1} e^{-\beta F(S)}\)</span>.</p>
</section>
<section id="global-and-detailed-balance-equations" class="level2">
<h2>Global and Detailed balance equations</h2>
<p>We can quite easily write down the properties that <span class="math inline">\(\mathcal{T}\)</span> must have in order to yield the correct target distribution. Since we must transition somewhere at each step, we first have the normalisation condition that <span class="math display">\[\sum\limits_S \mathcal{T}(S&#39; \rightarrow S) = 1.\]</span></p>
<p>Second, let us move to an ensemble view, where rather than individual walkers and states, we think about the probability distribution of many walkers at each step. If we start all the walkers in the same place the initial distribution will be a delta function and as we step the walkers will wander around, giving us a sequence of probability distributions <span class="math inline">\(\{p_0(S), p_1(S), p_2(S)\ldots\}\)</span>. For discrete spaces we can write the action of the transition function on <span class="math inline">\(p_i\)</span> as a matrix equation</p>
<p><span class="math display">\[\begin{aligned}
p_{i+1}(S) &amp;= \sum_{S&#39; \in \{S\}} p_i(S&#39;) \mathcal{T}(S&#39; \rightarrow S).
\end{aligned}\]</span></p>
<p>This equation is essentially just stating that total probability mass is conserved as our walkers flow around the state space.</p>
<p>In order that <span class="math inline">\(p_i\)</span> converges to our target distribution <span class="math inline">\(p\)</span> in the long time limit, we need the target distribution to be a fixed point of the transition function</p>
<p><span class="math display">\[\begin{aligned}
P(S) &amp;= \sum_{S&#39;} P(S&#39;) \mathcal{T}(S&#39; \rightarrow S).
\end{aligned}
\]</span> Along with some more technical considerations such as ergodicity which won’t be considered here, global balance suffices to ensure that a MCMC method is correct <span class="citation" data-cites="kellyReversibilityStochasticNetworks1981"> [<a href="#ref-kellyReversibilityStochasticNetworks1981" role="doc-biblioref">7</a>]</span>.</p>
<p>A sufficient but not necessary condition for global balance to hold is called detailed balance:</p>
<p><span class="math display">\[
P(S) \mathcal{T}(S \rightarrow S&#39;) = P(S&#39;) \mathcal{T}(S&#39; \rightarrow S).
\]</span></p>
<p>In practice most algorithms are constructed to satisfy detailed rather than global balance, though there are arguments that the relaxed requirements of global balance can lead to faster algorithms <span class="citation" data-cites="kapferSamplingPolytopeHarddisk2013"> [<a href="#ref-kapferSamplingPolytopeHarddisk2013" role="doc-biblioref">8</a>]</span>.</p>
<p>The goal of MCMC is then to choose <span class="math inline">\(\mathcal{T}\)</span> so that it has the desired thermal distribution <span class="math inline">\(P(S)\)</span> as its fixed point and converges quickly onto it. This boils down to requiring that the matrix representation of <span class="math inline">\(T_{ij} = \mathcal{T}(S_i \to S_j)\)</span> has an eigenvector with entries <span class="math inline">\(P_i = P(S_i)\)</span> with eigenvalue 1 and all other eigenvalues with magnitude less than one. The convergence time depends on the magnitude of the second largest eigenvalue.</p>
<p>The choice of the transition function for MCMC is under-determined as one only needs to satisfy a set of balance conditions for which there are many solutions <span class="citation" data-cites="kellyReversibilityStochasticNetworks1981"> [<a href="#ref-kellyReversibilityStochasticNetworks1981" role="doc-biblioref">7</a>]</span>. The standard choice that satisfies these requirements is called the Metropolis-Hastings algorithm.</p>
</section>
<section id="the-metropolis-hastings-algorithm" class="level2">
<h2>The Metropolis-Hastings Algorithm</h2>
<p>The Metropolis-Hastings algorithm breaks the transition function into a proposal distribution <span class="math inline">\(q(S \to S&#39;)\)</span> and an acceptance function <span class="math inline">\(\mathcal{A}(S \to S&#39;)\)</span>. <span class="math inline">\(q\)</span> must be a function we can directly sample from, and in many cases takes the form of flipping some number of spins in <span class="math inline">\(S\)</span>, i.e., if we are flipping a single random spin in the spin chain, <span class="math inline">\(q(S \to S&#39;)\)</span> is the uniform distribution on states reachable by one spin flip from <span class="math inline">\(S\)</span>. This also gives the symmetry property that <span class="math inline">\(q(S \to S&#39;) = q(S&#39; \to S)\)</span>.</p>
<p>The proposal <span class="math inline">\(S&#39;\)</span> is then accepted or rejected with an acceptance probability <span class="math inline">\(\mathcal{A}(S \to S&#39;)\)</span>, if the proposal is rejected then <span class="math inline">\(S_{i+1} = S_{i}\)</span>. Hence:</p>
<p><span class="math display">\[\mathcal{T}(S\to S&#39;) = q(S\to S&#39;)\mathcal{A}(S \to S&#39;).\]</span></p>
<p>The Metropolis-Hasting algorithm is a slight extension of the original Metropolis algorithm which allows for non-symmetric proposal distributions <span class="math inline">\(q(S\to S&#39;) \neq q(S&#39;\to S)\)</span>. It can be derived starting from detailed balance <span class="citation" data-cites="krauthIntroductionMonteCarlo1998"> [<a href="#ref-krauthIntroductionMonteCarlo1998" role="doc-biblioref">6</a>]</span>:</p>
<p><span class="math display">\[
P(S)\mathcal{T}(S \to S&#39;) = P(S&#39;)\mathcal{T}(S&#39; \to S),
\]</span></p>
<p>inserting the proposal and acceptance function</p>
<p><span class="math display">\[
P(S)q(S \to S&#39;)\mathcal{A}(S \to S&#39;) = P(S&#39;)q(S&#39; \to S)\mathcal{A}(S&#39; \to S),
\]</span></p>
<p>rearranging gives us a condition on the acceptance function in terms of the target distribution and the proposal distribution which can be thought of as inputs to the algorithm</p>
<p><span class="math display">\[
\frac{\mathcal{A}(S \to S&#39;)}{\mathcal{A}(S&#39; \to S)} = \frac{P(S&#39;)q(S&#39; \to S)}{P(S)q(S \to S&#39;)} = f(S, S&#39;).
\]</span></p>
<p>The Metropolis-Hastings algorithm is the choice</p>
<p><span class="math display">\[
\begin{aligned}
\label{eq:mh}
\mathcal{A}(S \to S&#39;) = \min\left(1, f(S,S&#39;)\right),
\end{aligned}
\]</span> for the acceptance function. The proposal distribution is left as a free choice.</p>
<p>Noting that <span class="math inline">\(f(S,S&#39;) = 1/f(S&#39;,S)\)</span>, we can see that the MH algorithm satisfies detailed balance by considering the two cases <span class="math inline">\(f(S,S&#39;) &gt; 1\)</span> and <span class="math inline">\(f(S,S&#39;) &lt; 1\)</span>.</p>
<p>By choosing the proposal distribution such that <span class="math inline">\(f(S,S&#39;)\)</span> is as close as possible to one, the rate of rejections can be reduced and the algorithm sped up. This can be challenging though, as getting <span class="math inline">\(f(S,S&#39;)\)</span> close to 1 would imply that we can directly sample from a distribution very close to the target distribution. As MCMC is usually applied to problems for which there is virtually no hope of sampling directly from the target distribution, it’s rare that one can do so approximately.</p>
<p>When the proposal distribution is symmetric as ours is, it cancels out in the expression for the acceptance function and the Metropolis-Hastings algorithm is simply the choice</p>
<p><span class="math display">\[
\mathcal{A}(S \to S&#39;) = \min\left(1, e^{-\beta\;\Delta F}\right),
\]</span></p>
<p>where <span class="math inline">\(F\)</span> is the overall free energy of the system, including both the quantum and classical sector.</p>
<p>To implement the acceptance function in practice we pick a random number in the unit interval and accept if it is less than <span class="math inline">\(e^{-\beta\;\Delta F}\)</span>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An implementation of the standard MH algorithm</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>current_state <span class="op">=</span> initial_state</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_steps):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    new_state <span class="op">=</span> proposal(current_state)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> free_energy_change(current_state, new_state, parameters)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> uniform(<span class="dv">0</span>,<span class="dv">1</span>) <span class="op">&lt;</span> exp(<span class="op">-</span>beta <span class="op">*</span> df):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        current_state <span class="op">=</span> new_state</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    states[i] <span class="op">=</span> current_state</span></code></pre></div>
</section>
<section id="app-mcmc-two-step-trick" class="level2">
<h2>Two Step Trick</h2>
<p>Our method already relies heavily on the split between the classical and quantum sector to derive a sign problem free MCMC algorithm but it turns out that there is a further trick we can play with it. The free energy term is the sum of an easy to compute classical energy and a more expensive quantum free energy, we can split the acceptance function into two in such a way as to avoid having to compute the full exact diagonalisation some of the time:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Our two step MH implementation for models with classical and quantum energy terms</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>current_state <span class="op">=</span> initial_state</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_steps):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    new_state <span class="op">=</span> proposal(current_state)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    df_classical <span class="op">=</span> classical_free_energy_change(current_state, new_state, parameters)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> exp(<span class="op">-</span>beta <span class="op">*</span> df_classical) <span class="op">&lt;</span> uniform(<span class="dv">0</span>,<span class="dv">1</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        f_quantum <span class="op">=</span> quantum_free_energy(current_state, new_state, parameters)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exp(<span class="op">-</span> beta <span class="op">*</span> df_quantum) <span class="op">&lt;</span> uniform(<span class="dv">0</span>,<span class="dv">1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>          current_state <span class="op">=</span> new_state</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        states[i] <span class="op">=</span> current_state</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div>
<p>As discussed in the main text, for the model parameters used, we find that with our new scheme the matrix diagonalisation is skipped around 30% of the time at <span class="math inline">\(T = 2.5\)</span> and up to 80% at <span class="math inline">\(T = 1.5\)</span>. We observe that for <span class="math inline">\(N = 50\)</span>, the matrix diagonalisation, if it occurs, occupies around 60% of the total computation time for a single step. This rises to 90% at N = 300 and further increases for larger N. We therefore get the greatest speedup for large system sizes at low temperature where many prospective transitions are rejected at the classical stage and the matrix computation takes up the greatest fraction of the total computation time. The upshot is that we find a speedup of up to a factor of 10 at the cost of very little extra algorithmic complexity.</p>
<p>This modified scheme has the acceptance function <span class="math display">\[\mathcal{A}(a \to b) = \min\left(1, e^{-\beta \Delta H_s}\right)\min\left(1, e^{-\beta \Delta F_c}\right).\]</span></p>
<p>We can show that this satisfies the detailed balance equations as follows. Defining <span class="math inline">\(r_c = e^{-\beta H_c}\)</span> and <span class="math inline">\(r_q = e^{-\beta F_q}\)</span> our target distribution is <span class="math inline">\(\pi(a) = r_c r_q\)</span>. This method has <span class="math inline">\(\mathcal{T}(a\to b) = q(a\to b)\mathcal{A}(a \to b)\)</span> with symmetric <span class="math inline">\(p(a \to b) = \pi(b \to a)\)</span> and <span class="math inline">\(\mathcal{A} = \min\left(1, r_c\right) \min\left(1, r_q\right)\)</span></p>
<p>Substituting this into the detailed balance equation gives: <span class="math display">\[\mathcal{T}(a \to b)/\mathcal{T}(b \to a) = \pi(b)/\pi(a) = r_c r_q.\]</span></p>
<p>Taking the LHS and substituting in our transition function: <span class="math display">\[\begin{aligned}
\mathcal{T}(a \to b)/\mathcal{T}(b \to a) = \frac{\min\left(1, r_c\right) \min\left(1, r_q\right)}{ \min\left(1, 1/r_c\right) \min\left(1, 1/r_q\right)},\end{aligned}\]</span></p>
<p>which simplifies to <span class="math inline">\(r_c r_q\)</span> as <span class="math inline">\(\min(1,r)/\min(1,1/r) = r\)</span> for <span class="math inline">\(r &gt; 0\)</span>.</p>
<section id="app-mcmc-autocorrelation" class="level3">
<h3>Autocorrelation Time</h3>
<figure>
<img src="/assets/thesis/fk_chapter/lsr/figs/m_autocorr.png" id="fig-m_autocorr" data-short-caption="Autocorrelation in MCMC" style="width:100.0%" alt="Figure 1: (Upper) 10 MCMC chains starting from the same initial state for a system with N = 150 sites and 3000 MCMC steps. At each MCMC step, n spins are flipped where n is drawn from Uniform(1,N) and this is repeated N^2/100 times. The simulations therefore have the potential to necessitate 10*N^2 matrix diagonalisations for each 100 MCMC steps. (Lower) The normalised autocorrelation (\langle m_i m_{i-j}\rangle - \langle m_i\rangle \langle m_i \rangle) / Var(m_i)) averaged over i. It can be seen that even with each MCMC step already being composed of many individual flip attempts, the autocorrelation is still non negligible and must be taken into account in the statistics. t = 1, \alpha = 1.25, T = 2.2, J = U = 5" />
<figcaption aria-hidden="true">Figure 1: (Upper) 10 MCMC chains starting from the same initial state for a system with <span class="math inline">\(N = 150\)</span> sites and 3000 MCMC steps. At each MCMC step, n spins are flipped where n is drawn from Uniform(1,N) and this is repeated <span class="math inline">\(N^2/100\)</span> times. The simulations therefore have the potential to necessitate <span class="math inline">\(10*N^2\)</span> matrix diagonalisations for each 100 MCMC steps. (Lower) The normalised autocorrelation <span class="math inline">\((\langle m_i m_{i-j}\rangle - \langle m_i\rangle \langle m_i \rangle) / Var(m_i))\)</span> averaged over <span class="math inline">\(i\)</span>. It can be seen that even with each MCMC step already being composed of many individual flip attempts, the autocorrelation is still non negligible and must be taken into account in the statistics. <span class="math inline">\(t = 1, \alpha = 1.25, T = 2.2, J = U = 5\)</span></figcaption>
</figure>
<p>At this stage one might think we are done. We can indeed draw independent samples from our target Boltzmann distribution by starting from some arbitrary initial state and doing <span class="math inline">\(k\)</span> steps to arrive at a sample. These are not, however, independent samples. In fig. <a href="#fig:m_autocorr">1</a> it is already clear that the samples of the order parameter <span class="math inline">\(m\)</span> have some autocorrelation because only a few spins are flipped each step. Even when the number of spins flipped per step is increased that it can be an important effect near the phase transition. Let’s define the autocorrelation time <span class="math inline">\(\tau(O)\)</span> informally as the number of MCMC samples of some observable O that are statistically equal to one independent sample or equivalently as the number of MCMC steps after which the samples are correlated below some cut-off, see ref. <span class="citation" data-cites="krauthIntroductionMonteCarlo1996"> [<a href="#ref-krauthIntroductionMonteCarlo1996" role="doc-biblioref">9</a>]</span>. The autocorrelation time is generally shorter than the convergence time so it therefore makes sense from an efficiency standpoint to run a single walker for many MCMC steps rather than to run a huge ensemble for <span class="math inline">\(k\)</span> steps each.</p>
<p>Once the random walk has been carried out for many steps, the expectation values of <span class="math inline">\(O\)</span> can be estimated from the MCMC samples <span class="math inline">\(S_i\)</span>: <span class="math display">\[
    \langle O \rangle = \sum_{i = 0}^{N} O(S_i) + \mathcal{O}(\frac{1}{\sqrt{N}}).
\]</span></p>
<p>The samples are correlated so the N of them effectively contains less information than <span class="math inline">\(N\)</span> independent samples would, in fact roughly <span class="math inline">\(N/\tau\)</span> effective samples. As a consequence the variance is larger than the <span class="math inline">\(\langle O^2 \rangle - \langle O \rangle ^2\)</span> form it would have if the estimates were uncorrelated. There are many methods in the literature for estimating the true variance of <span class="math inline">\(\langle O \rangle\)</span> and deciding how many steps are needed but my approach has been to run a small number of parallel chains, which are independent, in order to estimate the statistical error produced. This is a slightly less computationally efficient because it requires throwing away those <span class="math inline">\(k\)</span> steps generated before convergence multiple times but it is conceptually simple.</p>
</section>
<section id="tuning-the-proposal-distribution" class="level3">
<h3>Tuning the proposal distribution</h3>
<figure>
<img src="/assets/thesis/fk_chapter/lsr/figs/autocorr_multiple_proposals.png" id="fig-autocorr_multiple_proposals" data-short-caption="Comparison of different proposal distributions" style="width:100.0%" alt="Figure 2: Simulations showing how the autocorrelation of the order parameter depends on the proposal distribution used at different temperatures, we see that at T = 1.5 &lt; T_c a single spin flip is likely the best choice, while at the high temperature T = 2.5 &gt; T_c flipping two sites or a mixture of flipping two and 1 sites is likely a better choice. $t = 1, = 1.25, J = U = 5 $" />
<figcaption aria-hidden="true">Figure 2: Simulations showing how the autocorrelation of the order parameter depends on the proposal distribution used at different temperatures, we see that at <span class="math inline">\(T = 1.5 &lt; T_c\)</span> a single spin flip is likely the best choice, while at the high temperature <span class="math inline">\(T = 2.5 &gt; T_c\)</span> flipping two sites or a mixture of flipping two and 1 sites is likely a better choice. $t = 1, = 1.25, J = U = 5 $</figcaption>
</figure>
<p>Now we can discuss how to minimise the autocorrelations. The general principle is that one must balance the proposal distribution between two extremes. Choose overly small steps, like flipping only a single spin and the acceptance rate will be high because <span class="math inline">\(\Delta F\)</span> will usually be small, but each state will be very similar to the previous and the autocorrelations will be high too, making sampling inefficient. On the other hand, overlay large steps, like randomising a large portion of the spins each step, will result in very frequent rejections, especially at low temperatures.</p>
<p>I evaluated a few different proposal distributions for use with the FK model.</p>
<ol type="1">
<li>Flipping a single random site</li>
<li>Flipping N random sites for some N</li>
<li>Choosing n from Uniform(1, N) and then flipping n sites for some fixed N.</li>
<li>Attempting to tune the proposal distribution for each parameter regime.</li>
</ol>
<p>Fro fig. <a href="#fig:autocorr_multiple_proposals">2</a> we see that even at moderately high temperatures <span class="math inline">\(T &gt; T_c\)</span> flipping one or two sites is the best choice. However, for some simulations at very high temperature flipping more spins is warranted.</p>
<p>Next Section: <a href="../6_Appendices/A.3_Lattice_Generation.html">Lattice Generation</a></p>
</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">Bibliography</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-devroyeRandomSampling1986" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">L. Devroye, <em><a href="https://doi.org/10.1007/978-1-4613-8643-8_12">Random Sampling</a></em>, in <em>Non-Uniform Random Variate Generation</em>, edited by L. Devroye (Springer, New York, NY, 1986), pp. 611–641.</div>
</div>
<div id="ref-BMCP2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">O. A. Martin, R. Kumar, and J. Lao, <em>Bayesian Modeling and Computation in Python</em> (Boca Raton, 2021).</div>
</div>
<div id="ref-binderGuidePracticalWork1988" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">K. Binder and D. W. Heermann, <em><a href="https://doi.org/10.1007/978-3-662-08854-8_3">Guide to Practical Work with the Monte Carlo Method</a></em>, in <em>Monte Carlo Simulation in Statistical Physics: An Introduction</em>, edited by K. Binder and D. W. Heermann (Springer Berlin Heidelberg, Berlin, Heidelberg, 1988), pp. 68–112.</div>
</div>
<div id="ref-kerteszAdvancesComputerSimulation1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">J. Kertesz and I. Kondor, editors, <em><a href="https://doi.org/10.1007/BFb0105456">Advances in Computer Simulation: Lectures Held at the Eötvös Summer School in Budapest, Hungary, 16–20 July 1996</a></em> (Springer-Verlag, Berlin Heidelberg, 1998).</div>
</div>
<div id="ref-wolffMonteCarloErrors2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">U. Wolff, <em><a href="https://doi.org/10.1016/S0010-4655(03)00467-3">Monte Carlo Errors with Less Errors</a></em>, Computer Physics Communications <strong>156</strong>, 143 (2004).</div>
</div>
<div id="ref-krauthIntroductionMonteCarlo1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">W. Krauth, <em><a href="https://doi.org/10.1007/BFb0105456">Introduction To Monte Carlo Algorithms</a></em>, in <em>Advances in Computer Simulation: Lectures Held at the Eötvös Summer School in Budapest, Hungary, 16–20 July 1996</em> (Springer-Verlag, Berlin Heidelberg, 1998).</div>
</div>
<div id="ref-kellyReversibilityStochasticNetworks1981" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">F. P. Kelly, <em><a href="https://doi.org/10.2307/2287860">Reversibility and Stochastic Networks / F.P. Kelly</a></em>, SERBIULA (Sistema Librum 2.0) <strong>76</strong>, (1981).</div>
</div>
<div id="ref-kapferSamplingPolytopeHarddisk2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">S. C. Kapfer and W. Krauth, <em><a href="https://doi.org/10.1088/1742-6596/454/1/012031">Sampling from a Polytope and Hard-Disk Monte Carlo</a></em>, J. Phys.: Conf. Ser. <strong>454</strong>, 012031 (2013).</div>
</div>
<div id="ref-krauthIntroductionMonteCarlo1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">W. Krauth, <em><a href="http://arxiv.org/abs/cond-mat/9612186">Introduction To Monte Carlo Algorithms</a></em>, arXiv:cond-Mat/9612186 (1996).</div>
</div>
</div>
</section>


</main>
</body>
</html>
